# LLM (대규모 언어 모델)
> LLM (대규모 언어 모델) 이란, 자연어 데이터를 기반으로 학습되어 인간의 언어 사용 패턴을 모방하고 텍스트를 생성하며, 질문에 답하고, 번역하는 등 다양한 자연어 처리(NLP) 작업을 수행할 수 있는 인공지능 모델
- 각 토큰이 생성될 때, 이전 토큰들이 다음 토큰을 생성하는 데 필요한 컨텍스트로 계속해서 추가

## 주요 특징 및 기능
- 텍스트 생성: LLM의 가장 기본적인 기능으로, 주어진 입력(프롬프트)에 이어서 자연스러운 텍스트 생성
  - 모델이 학습된 데이터의 패턴을 기반으로 다음에 올 확률이 높은 단어를 예측하고 연속적으로 이어 붙이는 방식
  - 대부분의 LLM은 "좌측-우측 LLM(Left-to-right LLM)", "인과적 LLM(Causal LLM)" 또는 "자기회귀적 LLM(Autoregressive LLM)" 라고 칭함
- 다양한 NLP 작업 수행: 많은 실제 자연어 처리(NLP) 작업을 단어 예측 작업으로 전환하여 수행
  - 예) 감정분석, 질의응답, 요약
  - 주어진 정보(이전 단어들)를 바탕으로 다음에 올 가장 적절한 단어를 끊임없이 예측하고 채워나가는 과정을 반복하여, 전체적인 문장이나 문서, 질문에 대한 답변까지 완성하는 구조
- 추론 능력: 연쇄적 사고 (Chain-of-Thought, CoT) 프롬프팅과 같은 기법을 통해 LLM은 문제 해결 과정을 여러 단계로 나누어 "생각" 하는 능력 발휘 -> 복잡하고 논리적인 작업 해결 가능
  - 단순히 텍스트를 생성하는 것을 넘어, 복잡한 문제 해결을 위한 추론 능력 지님
  - 추론 과정에서 오류 전파 가능
- 파인튜닝(Finetuning): 사전 훈련된 모델의 모든 또는 일부 파라미터를 새로운 데이터에 추가로 학습시켜 새로운 도메인이나 언어에 적응
  - 매개변수를 조정하여 새로운 데이터에 잘 적응하도록 하는 과정
- 규모 의존성: 모델 크기(파라미터 수), 데이터셋 크기, 컴퓨팅 예산과 같은 요소에 따라 거듭제곱 법칙으로 성능 확장 및 손실 감소

## LLM의 한계 및 문제점:
1. 추론 및 인지 능력의 한계
  - 환각 (Hallucination): LLM은 사실과 다른 정보를 생성할 수 있음. 특히, Chain-of-Thought 추론 방식에서 오류 전파로 이어질 수 있음.
  - 복잡한 추론의 어려움
  - 내부 지식의 한계: LLM은 훈련 데이터의 지식에 기반
2. 메모리 및 컨텍스트 관리의 어려움
  - 제한된 컨텍스트 윈도우: 장기간의 상호작용에서 일관된 상태 정보 유지 어려움
3. 운영 및 효율성 문제
  - 높은 지연 시간 및 비용
  - 프롬프트 중복: 관찰에 의존하는 추론 방식(ex. ReAct)은 외부 도구의 응답을 기다리며 LLM이 일시 중단되며, 다음 토큰 생성을 위해 모든 이전 컨텍스트가 반복적으로 LLM에 재입력되면서 상당한 프롬프트 중복 발생
  - 불안정한 동작: 무한 루프에 빠질 수 있음
  - 오버헤드 및 병렬화의 한계
  - 다중 에이전트 시스템의 복잡성과 조정 문제
4. 윤리 및 사회적 문제
  - 저작권 문제: LLM의 훈련 데이터에 저작권이 있는 텍스트가 대량으로 포함되어 있어, 이러한 사용이 공정 사용 원칙에 부합하는지에 대한 법적 불확실성 존재
  - 개인 정보 보호: 개인 정보 유출의 위험
  - 유해성 및 편향
  - 통제 및 책임 문제
5. 학습 및 일반화의 어려움
  - 파인튜닝의 복잡성
  - 실제 적용의 어려움

<br/>
<br/>
<br/>
출처

- [LargeLanguageModels](https://web.stanford.edu/~jurafsky/slp3/slides/LLM24aug.pdf)
- AI Agents: Evolution, Architecture, and Real-World Applications/Naveen Krishnan/March 2025
