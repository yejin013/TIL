<img width="268" height="274" alt="image" src="https://github.com/user-attachments/assets/3bb7d9e5-397d-49c4-b23d-0611428d1a2a" /># 트랜스포머 아키텍처
> 트랜스포머 아키텍처란, 순차적 데이터를 처리하는 데 탁월한, 언어 번역을 위한 딥러닝 모델

## 기존의 seq2seq 모델의 한계
- 인코더가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실된다는 단점
- 이를 보정하기 위해 어텐션 사용
- 트랜스포머 : RNN 보정을 위해 어텐션을 사용하는 것이 아니라, 어텐션만으로 인코더와 디코더 생성

## 트랜스포머 구성
<img width="307" height="351" alt="image" src="https://github.com/user-attachments/assets/52efc912-aa24-4d07-9c29-1f18ebfd938e" />

1. 인코더 (Encoder) : 입력 텍스트를 처리하고 텍스트의 임베딩 표현(다양한 차원에서 여러 가지 요소를 인식하는 수치 표현)을 생성하는 인코더
2. 디코더 (Decoder) : 번역된 텍스트를 한 단어씩 생성하기 위해 인코더가 생성한 임베딩 표현을 사용하는 디코더
ex. 번역 작업에서는 인코더가 소스 언어의 텍스트를 벡터로 인코딩하고, 디코더가 이 벡터를 디코딩하여 대상 언어의 텍스트 생성

## 트랜스포머 핵심 구성 요소 
### 셀프 어텐션 매커니즘 (self-attention)
<img width="268" height="274" alt="image" src="https://github.com/user-attachments/assets/10326d42-3241-4eb5-9e6d-f97330fb38f5" />

- 셀프 어텐션 : Query, Key, Value가 동일한 경우 (벡터의 출처가 같음!!! - 벡터의 값 같다는 의미X)
- 인코더의 셀프 어텐션 : Query = Key = Value
- 디코더의 마스크드 셀프 어텐션 : Query = Key = Value
- 디코더의 인코더-디코더 어텐션 (셀프 어텐션 X) : Query : 디코더 벡터 / Key = Value : 인코더 벡터

- Q : 입력 문장의 모든 단어 벡터들
- K : 입력 문장의 모든 단어 벡터들
- V : 입력 문장의 모든 단어 벡터들

### 멀티헤드 어텐션 (Multi-Head Attention)
- 어텐션을 병렬로 여러 개 수행하는 기법으로, 입력을 여러 개의 "헤드(head)"로 분할해 다양한 관점의 어텐션을 수행한 뒤 이를 다시 결합
- 각각의 헤드는 입력 벡터에 대해 서로 다른 Query, Key, Value 행렬을 학습하며, 어텐션 스코어를 통해 중요한 부분에 집중
- 최종적으로 각 헤드의 출력을 concatenate하여 다시 하나의 벡터로 합친 뒤, 선형 변환을 통해 원래 차원으로 돌림

### 포지셔널 인코딩 (Positional Encoding)
- 어텐션 메커니즘 자체에는 단어 위치에 대한 정보가 없기 때문에, 시퀀스 상에서 토큰의 순서를 알려주는 것이 중요
-  포지셔널 인코딩 : 입력 임베딩에 위치 정보를 담은 벡터 더함
-  임베딩 벡터의 값 범위를 교란시키지 않도록 적절히 스케일 조정한 후 더해주며, 모델이 위치에 따른 규칙성 학습 가능

### 포지션-와이즈 피드포워드 네트워크 (FFN)
- 어텐션으로 문맥이 반영된 각 토큰의 표현은 피드포워드 신경망을 통해 한 번 더 변환
- 토큰 자체의 표현을 깊게 변형하는 역할

### 레이어 정규화 (Layer Normalization) 및 잔차 연결 (Residual Connection)


## 트랜스포머 아키텍처 동작방식
1. 문장을 토큰(단어 조각)으로 자름
2. 각 토큰을 벡터(숫자들)로 바꿈
3. 이 벡터들을 self-attention으로 서로 관계 파악
4. FFN으로 각 토큰을 깊이 있게 처리
5. 위 과정을 여러 층(stack) 반복
6. 최종적으로 문장의 의미를 이해하거나, 새로운 문장을 생성함

## 특징
- RNN처럼 순서대로 처리하지 않고, 병렬로 빠르게 학습 가능
- 문맥을 길게 기억할 수 있음
  
<br/>
<br/>
<br/>
출처

- [트랜스포머 모델이란 무엇인가요?](https://www.ibm.com/kr-ko/think/topics/transformer-model)
- [트렌스포머 아키텍처 소개](https://wikidocs.net/253580)
- [트랜스포머(Transformer)](https://wikidocs.net/31379)
